# Design

These are the primary design constraints:

* We may receive logging events on any thread, and don't want to block the thread while communicating
  with the service.
* Services may reject our requests, either temporarily or permanently.
* We generally want to batch individual messages to improve throughput, respecting any limits on
  number of messages or bytes that are imposed by the service.
* The amount of time taken to build a batch should be controllable by the user, as a trade-off
  between efficiency and potential for lost messages.


## Message Queue and Writer Thread

To meet these constraints, the appender creates a separate thread for communication with the service,
along with a concurrent queue to hold appended messages. When Log4J calls `append()`, the appender
converts the passed `LoggingEvent` into a  textual representation, verifies that it conforms to
limitations imposed by the service, and adds it to the queue.

The writer consumes that queue, attempting to batch together messages into a single request. Once it
has a batch (either based on size or a configurable timeout) it attempts to write those messages to
the service. In addition to retries embedded within the AWS SDK, the writer will requeue messages
that can't be sent, dropping messages once a user-configurable threshold is reached.

The writer thread is lazily started on the first call to `append()`. There's a factory for writer
objects and writer threads, to support testing. If unable to start the writer thread, all messages
are dropped and the situation is reported to the internal logger.

The writer thread handles most exceptions internally. Unexpected exceptions are reported using an
uncaught exception handler in the appender. This is considered an unrecoverable error, so the
appender discards its writer, clears the message queue, and drops all subsequent messages. The
error is reported to the internal logger and is also available from [JMX](jmx.md).


## Message Discard

The appenders will attempt to deliver every message, requeuing messages that can't be sent (this is
particularly relevant to the Kinesis appender, since some messages in a batch may fail while others
succeed). If there are persistent errors, such as a network outage, an unconstrained queue could
consume all of the available memory.

To avoid such errors, the message queue has a maximum size, configured by the `discardThreshold`
parameter. How it behaves once full is configured by the `discardAction` parameter:

* `oldest` - the oldest message in the queue is discarded. This is the default, as it allows you to
  track the current behavior of the application once the failure condition is resolved.
* `newest` - the newest message in the queue is discarded. This is useful if you want to see what
  was happening at the time the failure condition occured.
* `none` - no messages are discarded. If you expect intermittent connectivity problems, have lots of
  memory, and don't want to miss any logging then this option may be reasonable. However, it's probably
  better to increase the threshold and use one of the other discard actions.

The default threshold is 10,000 messages. Assuming 1kb per message, that's 10MB of heap that will be
used by the queue. 


## Message Batches

Most AWS services allow batching of messages for efficiency. While sending maximum-sized requests is
more efficient when there's a high volume of logging, it would cause an excessive delay if there's a
low volume. And it will leave more messages unwritten if the program shuts down without waiting for
all messages to be sent.

The user can control message batching via the `batchDelay` configuration variable, which specifies the
number of milliseconds that the writer will wait after reading a message to construct a batch. The
writer will send the batch either once the timer expires or the service-defined batch size limit is
reached. Then it blocks, waiting for a message to start the next batch.

The default value, 2000, is intended as a tradeoff between keeping the log up to date and minimizing
the number of API calls generated by the logger (to avoid throttling). For long-running applications
this default should be fine, but for applications that only run for a few seconds it may cause message
loss (the appenders do not attach a shutdown hook, and the JVM does not necessarily respect those anyway).
For such applications it makes sense to reduce the batch size to maybe 250 milliseconds, but beware that
increasing the message rate may result in AWS throttling requests, which could extend the actual delay
between a logging event and that event being written to its destination.

If you absolutely, positively cannot lose messages, you should use a different appender. But beware:
even the standard `FileAppender` is not guaranteed to save all messages, because file writes are
buffered in memory before they're actually written to the disk.
