# Design

The primary design constraints are these:

* We may receive logging events on any thread, and don't want to block the thread while communicating
  with the service.
* We generally want to batch individual messages to improve throughput, although the service may have
  constraints on either number of messages or bytes in a batch.
* Services may reject our requests, either temporarily or permanently.

To meet these constraints, the appender creates a separate thread for communication with the service,
with a concurrent queue to hold appended messages. When Log4J calls `append()`, the appender converts
the passed `LoggingEvent` into a textual representation, verifies that it conforms to limitations imposed
by the service, and adds it to the queue.

The writer consumes that queue, attempting to batch together messages into a single request. Once it
has a batch (either based on size or a configurable timeout) it attempts to write those messages to
the service. In addition to retries embedded within the AWS SDK, the writer will requeue messages
that can't be sent, dropping messages once a user-configurable threshold is reached.

The writer thread is lazily started on the first call to `append()`. There's a factory for writer
objects and writer threads, to support testing. If unable to start the writer thread, messages are
dropped and the situation is reported to the internal logger.

The writer thread handles most exceptions internally. Unexpected exceptions are reported using an
uncaught exception handler in the appender. This will trigger the appender to discard the writer
and create a new one (and also to report the failure to the internal logger).

The writer uses the default constructor for each AWS service, which in turn uses the default credential
provider chain. This allows you to specify explicit credentials using several mechanisms, or to use
instance roles for applications running on EC2 or Lambda.

## Message Batches

Most AWS services allow batching of messages for efficiency. While sending maximum-sized requests is
more efficient when there's a high volume of logging, it could excessively delay writing when there's
a low volume (and potentially leave more messages unwritten if the program crashes).

To support this, the appenders provide the `batchDelay` configuration variaable. When the first message
in a batch is pulled off the internal queue, the writer starts a timer with this value. The writer will
read additional messages until it either fills the batch or the timer is at zero, at which point it
sends the batch and starts a new one.

This timeout is also used as a "cooldown" timer when the writer is closed (as when the appender rotates
its log stream): the writer will continue to wait for messages for this amount of time. Note that the writer
might actually take longer to shut down, if there is a large backlog of messages or communication errors
that prevent the batch being written.

The default value, 2000, is intended as a tradeoff between keeping the log up to date and minimizing the amount
of network traffic generated by the logger.

## Message Discard

The appenders will attempt to deliver every message, requeing the messages if they fail (this is particuarly
relevant to the Kinesis appender, since some messages in a batch may fail while others succeed). If there
are persistent errors, such as a network outage, this could cause an out-of-memory situation.

To avoid such errors you can configure `discardThreshold` and `discardAction` parameters, which control how
messages are discarded. The threshold controls the maximum number of messages that will be maintained; once
that threshold is crossed, messages will be discarded according to one of the following rules:

* `oldest` - the oldest message in the queue is discarded; this is the default, as it allows you to track
  the current behavior of the application once the failure condition is resolved.
* `newest` - the newest message in the queue is discarded. This is useful if you want to see what's
  happening at the time the failure condition occurs.
* `none` - no messages are discarded. If you expect intermittent connectivity problems, have lots of
  memory, and don't want to miss any logging then this option may be reasonable. However, it's probably
  better to increase the threshold and use one of the other discard actions.

The default threshold is 10,000 messages. Assuming 1kb per message, that's 10MB of heap that will be used
by the queue. 
